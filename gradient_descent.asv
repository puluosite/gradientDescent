clear all;
clc;

%% old gradient descent
err = 1.0;
lamda = 1e-4;
max_iter = 300;
iter = 1;
all_error = [];
all_y = [];

init_x = 0;
step = 0.01;% when step = 1, init_x = 0, wierd thing happens, 
while (err > lamda && iter <= max_iter)
    [y, gradient] = func(init_x);
    % fixed step
    new_x = init_x - gradient*step;
    [new_y, new_gradient] = func(new_x);
    new_err = abs(new_y - y)^2;
    all_error = [all_error, new_err];
    all_y = [all_y, new_y];
    err = new_err;
    init_x = new_x;
end

plot(all_error);